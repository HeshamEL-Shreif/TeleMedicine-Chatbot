# ğŸ§  TeleMedicine Chatbot â€“ Backend (FastAPI + LangGraph)

This repository contains the **FastAPI backend** for a **multilingual AI-powered TeleMedicine Chatbot**. The system uses **LangChain**, **LangGraph**, and **Groqâ€™s LLaMA 4 model** for reasoning over internal medical documents and relevant web data via **Tavily search**.

---

## ğŸ©º Features

- ğŸ” **Medical QA with Retrieval-Augmented Generation (RAG)**
- ğŸŒ **Arabic & English** support with dynamic language detection
- ğŸ“‘ **Synthetic paragraph generation** for document-based semantic search
- ğŸ›  **LangGraph agent** for tool selection and reasoning
- ğŸ“š **Chroma vector store** for persistent document storage
- ğŸŒ **Web search integration** via Tavily
- ğŸš€ **FastAPI server** with a clean REST endpoint
- ğŸ³ **Docker-ready** for deployment

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ db/                            # Vector DB logic (Chroma)
â”‚   â””â”€â”€ vector_db.py
â”œâ”€â”€ documents/                    # Your internal medical PDFs
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ utils.py                  # llm and search_tool definitions
â”œâ”€â”€ retrieve.py                   # Tool that generates synthetic Arabic text
â”œâ”€â”€ agent.py                      # LangGraph workflow agent
â”œâ”€â”€ main.py                       # FastAPI app with /query endpoint
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```
## ğŸ§  LLM & Tools
- LLM: meta-llama/llama-4-scout-17b-16e-instruct (served by Groq)
- Embedding Model: sentence-transformers paraphrase-multilingual-MiniLM-L12-v2
- Search Tool: Tavily API
- RAG Workflow: Powered by LangGraph

## ğŸ§ª Example API Call

- Endpoint: POST /query

### Request

```json
{
  "query": "Ù…Ø§ Ù‡Ùˆ Ø¯ÙˆØ§Ø¡ Ø§Ù„Ø¯Ø§ÙƒØªÙˆÙ†ØŸ"
}
```

### Response

```json
{
  "answer": "Ø§Ù„Ø¯Ø§ÙƒØªÙˆÙ† (Ø³Ø¨ÙŠØ±ÙˆÙ†ÙˆÙ„Ø§ÙƒØªÙˆÙ†) Ù‡Ùˆ Ø¯ÙˆØ§Ø¡ Ù…Ø¯Ø± Ù„Ù„Ø¨ÙˆÙ„ ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙˆØªØ§Ø³ÙŠÙˆÙ… ÙˆÙŠØ³ØªØ®Ø¯Ù… Ù„Ø¹Ù„Ø§Ø¬ Ø§Ø­ØªØ¨Ø§Ø³ Ø§Ù„Ø³ÙˆØ§Ø¦Ù„ Ø§Ù„Ù†Ø§ØªØ¬ Ø¹Ù† Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù‚Ù„Ø¨ ÙˆØ§Ù„ÙƒØ¨Ø¯ ÙˆØ§Ù„ÙƒÙ„Ù‰..."
}
```

## ğŸ§° Setup Instructions

1. Clone the Repo
```bash
git clone https://github.com/yourusername/telemedicine-chatbot-backend.git
cd telemedicine-chatbot-backend
```
2. Install Dependencies
```bash
pip install -r requirements.txt
```
3. Create a .env file and add:
```bash
TAVILY_API_KEY=your_api_key_here
HUGGINGFACEHUB_API_TOKEN =your_api_key_here
GROQ_API_KEY=your_api_key_here
```
4. Run the API
```bash
uvicorn main:app --reload
```
5. Visit API docs: http://localhost:8000/docs

## ğŸ³ Docker Usage
1. Build Docker Image
```bash
docker build -t telemedicine-backend .
```
2. Run the Container
```bash
docker run -p 8000:8000 telemedicine-backend
```

## ğŸ§  How It Works
1.	User query is received.
2.	Query is language detected (Arabic or English).
3.	A synthetic paragraph is generated using LLM to simulate an answer-containing doc.
4.	Similarity search runs on that paragraph against internal medical vector DB.
5.	Web search snippets are fetched via Tavily (if relevant).
6.	LangGraph agent uses tools + context to generate a final answer.
7.	Final answer is returned in the userâ€™s language, avoiding direct quoting.

## ğŸ§  Prompt Logic Highlights
- Responds only with medical facts
- Ignores or filters non-medical questions
- Detects and replies in the userâ€™s language
- Prefers internal documents, then supplements with web content
- Avoids hallucinations and general-purpose LLM errors